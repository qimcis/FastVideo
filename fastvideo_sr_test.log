INFO 10-30 00:36:45 [__init__.py:109] ROCm platform is unavailable: No module named 'amdsmi'
WARNING 10-30 00:36:45 [logger.py:122] [92m By default, logger.info(..) will only log from the local main process. Set logger.info(..., is_local_main_process=False) to log from all processes.[0;0m
INFO 10-30 00:36:45 [__init__.py:47] CUDA is available
INFO 10-30 00:36:46 [utils.py:591] Diffusers version: 0.33.0.dev0
WARNING 10-30 00:36:46 [registry.py:119] Trying to use the config from the model_index.json. FastVideo may not correctly identify the optimal config for this model in this situation.
WARNING 10-30 00:36:46 [registry.py:132] No match found for pipeline /workspace/models/FastWan2.1-14B-Diffusers, using fallback config <class 'fastvideo.configs.pipelines.wan.WanT2V480PConfig'>.
INFO 10-30 00:36:46 [multiproc_executor.py:41] Use master port: 51139
INFO 10-30 00:36:50 [__init__.py:109] ROCm platform is unavailable: No module named 'amdsmi'
WARNING 10-30 00:36:50 [logger.py:122] [92m By default, logger.info(..) will only log from the local main process. Set logger.info(..., is_local_main_process=False) to log from all processes.[0;0m
INFO 10-30 00:36:50 [__init__.py:47] CUDA is available
INFO 10-30 00:36:50 [__init__.py:109] ROCm platform is unavailable: No module named 'amdsmi'
WARNING 10-30 00:36:50 [logger.py:122] [92m By default, logger.info(..) will only log from the local main process. Set logger.info(..., is_local_main_process=False) to log from all processes.[0;0m
INFO 10-30 00:36:50 [__init__.py:47] CUDA is available
INFO 10-30 00:36:52 [parallel_state.py:976] Initializing distributed environment with world_size=2, device=cuda:0
INFO 10-30 00:36:52 [parallel_state.py:788] Using nccl backend for CUDA platform
INFO 10-30 00:36:52 [parallel_state.py:976] Initializing distributed environment with world_size=2, device=cuda:1
INFO 10-30 00:36:52 [utils.py:83] Found nccl from library libnccl.so.2
INFO 10-30 00:36:52 [pynccl.py:68] FastVideo is using nccl==2.26.2
INFO 10-30 00:36:53 [utils.py:83] Found nccl from library libnccl.so.2
INFO 10-30 00:36:53 [pynccl.py:68] FastVideo is using nccl==2.26.2
INFO 10-30 00:36:53 [utils.py:512] Model already exists locally at /workspace/models/FastWan2.1-14B-Diffusers
INFO 10-30 00:36:53 [__init__.py:43] Model path: /workspace/models/FastWan2.1-14B-Diffusers
INFO 10-30 00:36:53 [utils.py:591] Diffusers version: 0.33.0.dev0
INFO 10-30 00:36:53 [__init__.py:53] Building pipeline of type: basic
INFO 10-30 00:36:53 [pipeline_registry.py:162] Loading pipelines for types: ['basic']
INFO 10-30 00:36:53 [pipeline_registry.py:218] Loaded 8 pipeline classes across 1 types
INFO 10-30 00:36:53 [profiler.py:191] Torch profiler disabled; returning no-op controller
INFO 10-30 00:36:53 [composed_pipeline_base.py:83] Loading pipeline modules...
INFO 10-30 00:36:53 [utils.py:512] Model already exists locally at /workspace/models/FastWan2.1-14B-Diffusers
INFO 10-30 00:36:53 [composed_pipeline_base.py:213] Model path: /workspace/models/FastWan2.1-14B-Diffusers
INFO 10-30 00:36:53 [utils.py:591] Diffusers version: 0.33.0.dev0
INFO 10-30 00:36:53 [composed_pipeline_base.py:273] Loading pipeline modules from config: {'_class_name': 'WanPipeline', '_diffusers_version': '0.33.0.dev0', 'scheduler': ['diffusers', 'UniPCMultistepScheduler'], 'text_encoder': ['transformers', 'UMT5EncoderModel'], 'tokenizer': ['transformers', 'T5TokenizerFast'], 'transformer': ['diffusers', 'WanTransformer3DModel'], 'vae': ['diffusers', 'AutoencoderKLWan']}
INFO 10-30 00:36:53 [composed_pipeline_base.py:297] SR mode enabled. Adding transformer_2 to required modules.
INFO 10-30 00:36:53 [composed_pipeline_base.py:329] Loading required modules: ['text_encoder', 'tokenizer', 'vae', 'transformer', 'scheduler', 'transformer_2']
INFO 10-30 00:36:53 [component_loader.py:601] Loading scheduler using diffusers from /workspace/models/FastWan2.1-14B-Diffusers/scheduler
INFO 10-30 00:36:53 [composed_pipeline_base.py:363] Loaded module scheduler from /workspace/models/FastWan2.1-14B-Diffusers/scheduler
INFO 10-30 00:36:53 [component_loader.py:601] Loading text_encoder using transformers from /workspace/models/FastWan2.1-14B-Diffusers/text_encoder
INFO 10-30 00:36:53 [component_loader.py:223] HF Model config: {'architectures': ['UMT5EncoderModel'], 'classifier_dropout': 0.0, 'd_ff': 10240, 'd_kv': 64, 'd_model': 4096, 'decoder_start_token_id': 0, 'dense_act_fn': 'gelu_new', 'dropout_rate': 0.1, 'eos_token_id': 1, 'feed_forward_proj': 'gated-gelu', 'initializer_factor': 1.0, 'is_encoder_decoder': True, 'is_gated_act': True, 'layer_norm_epsilon': 1e-06, 'num_decoder_layers': 24, 'num_heads': 64, 'num_layers': 24, 'output_past': True, 'pad_token_id': 0, 'relative_attention_max_distance': 128, 'relative_attention_num_buckets': 32, 'scalable_attention': True, 'tie_word_embeddings': False, 'use_cache': True, 'vocab_size': 256384}
Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.42it/s]
Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.00it/s]
Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:03<00:02,  1.05s/it]
Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:03<00:00,  1.06it/s]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:04<00:00,  1.07it/s]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:04<00:00,  1.06it/s]

INFO 10-30 00:36:58 [component_loader.py:270] Loading weights took 5.01 seconds
INFO 10-30 00:37:35 [composed_pipeline_base.py:363] Loaded module text_encoder from /workspace/models/FastWan2.1-14B-Diffusers/text_encoder
INFO 10-30 00:37:35 [component_loader.py:601] Loading tokenizer using transformers from /workspace/models/FastWan2.1-14B-Diffusers/tokenizer
INFO 10-30 00:37:35 [component_loader.py:374] Loading tokenizer from /workspace/models/FastWan2.1-14B-Diffusers/tokenizer
INFO 10-30 00:37:36 [component_loader.py:383] Loaded tokenizer: T5TokenizerFast
INFO 10-30 00:37:36 [composed_pipeline_base.py:363] Loaded module tokenizer from /workspace/models/FastWan2.1-14B-Diffusers/tokenizer
INFO 10-30 00:37:36 [component_loader.py:601] Loading transformer using diffusers from /workspace/models/FastWan2.1-14B-Diffusers/transformer
INFO 10-30 00:37:36 [component_loader.py:440] Loading transformer with transformer cls_name: WanTransformer3DModel
INFO 10-30 00:37:36 [component_loader.py:480] Loading model from 2 safetensors files: ['/workspace/models/FastWan2.1-14B-Diffusers/transformer/diffusion_pytorch_model-00001-of-00002.safetensors', '/workspace/models/FastWan2.1-14B-Diffusers/transformer/diffusion_pytorch_model-00002-of-00002.safetensors']
INFO 10-30 00:37:36 [component_loader.py:487] Loading model from WanTransformer3DModel, default_dtype: torch.bfloat16
INFO 10-30 00:37:36 [fsdp_load.py:93] Loading model with default_dtype: torch.bfloat16
INFO 10-30 00:37:36 [cuda.py:124] Trying FASTVIDEO_ATTENTION_BACKEND=VIDEO_SPARSE_ATTN
INFO 10-30 00:37:36 [cuda.py:126] Selected backend: AttentionBackendEnum.VIDEO_SPARSE_ATTN
INFO 10-30 00:37:36 [cuda.py:176] Using Video Sparse Attention backend.
INFO 10-30 00:37:36 [cuda.py:124] Trying FASTVIDEO_ATTENTION_BACKEND=VIDEO_SPARSE_ATTN
INFO 10-30 00:37:36 [cuda.py:126] Selected backend: None
INFO 10-30 00:37:36 [cuda.py:251] Using Flash Attention backend.
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 37.23it/s]

INFO 10-30 00:37:52 [component_loader.py:514] Loaded model with 15.34B parameters
INFO 10-30 00:37:52 [composed_pipeline_base.py:363] Loaded module transformer from /workspace/models/FastWan2.1-14B-Diffusers/transformer
INFO 10-30 00:37:52 [component_loader.py:601] Loading vae using diffusers from /workspace/models/FastWan2.1-14B-Diffusers/vae
INFO 10-30 00:37:52 [composed_pipeline_base.py:363] Loaded module vae from /workspace/models/FastWan2.1-14B-Diffusers/vae
INFO 10-30 00:37:52 [composed_pipeline_base.py:377] Loading SR rendering transformer from /workspace/models/FastWan2.1-1.3B-Diffusers
INFO 10-30 00:37:52 [component_loader.py:601] Loading transformer_2 using diffusers from /workspace/models/FastWan2.1-1.3B-Diffusers/transformer
INFO 10-30 00:37:52 [component_loader.py:440] Loading transformer_2 with transformer cls_name: WanTransformer3DModel
INFO 10-30 00:37:52 [component_loader.py:480] Loading model from 1 safetensors files: ['/workspace/models/FastWan2.1-1.3B-Diffusers/transformer/diffusion_pytorch_model.safetensors']
INFO 10-30 00:37:52 [component_loader.py:487] Loading model from WanTransformer3DModel, default_dtype: torch.bfloat16
INFO 10-30 00:37:52 [fsdp_load.py:93] Loading model with default_dtype: torch.bfloat16
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 36.80it/s]

INFO 10-30 00:37:54 [component_loader.py:514] Loaded model with 1.49B parameters
INFO 10-30 00:37:54 [__init__.py:67] Pipelines instantiated
[1;36m(Worker pid=13636)[0;0m INFO 10-30 00:37:54 [multiproc_executor.py:445] Worker 0 starting event loop...
INFO 10-30 00:37:55 [multiproc_executor.py:436] 2 workers ready
INFO 10-30 00:37:55 [utils.py:591] Diffusers version: 0.33.0.dev0
WARNING 10-30 00:37:55 [registry.py:90] FastVideo may not correctly identify the optimal sampling param for this model, as the local directory may have been renamed.
WARNING 10-30 00:37:55 [registry.py:115] No match found for pipeline /workspace/models/FastWan2.1-14B-Diffusers, using fallback sampling param <class 'fastvideo.configs.sample.wan.WanT2V_1_3B_SamplingParam'>.
INFO 10-30 00:37:55 [video_generator.py:347] Adjusting number of frames from 81 to 85 based on number of GPUs (2)
INFO 10-30 00:37:55 [video_generator.py:380] 
INFO 10-30 00:37:55 [video_generator.py:380]                       height: 720
INFO 10-30 00:37:55 [video_generator.py:380]                        width: 1280
INFO 10-30 00:37:55 [video_generator.py:380]                 video_length: 85
INFO 10-30 00:37:55 [video_generator.py:380]                       prompt: Blonde girl walking in central park
INFO 10-30 00:37:55 [video_generator.py:380]                       image_path: None
INFO 10-30 00:37:55 [video_generator.py:380]                   neg_prompt: Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
INFO 10-30 00:37:55 [video_generator.py:380]                         seed: 1024
INFO 10-30 00:37:55 [video_generator.py:380]                  infer_steps: 50
INFO 10-30 00:37:55 [video_generator.py:380]        num_videos_per_prompt: 1
INFO 10-30 00:37:55 [video_generator.py:380]               guidance_scale: 3.0
INFO 10-30 00:37:55 [video_generator.py:380]                     n_tokens: 316800
INFO 10-30 00:37:55 [video_generator.py:380]                   flow_shift: 3.0
INFO 10-30 00:37:55 [video_generator.py:380]      embedded_guidance_scale: 6.0
INFO 10-30 00:37:55 [video_generator.py:380]                   save_video: True
INFO 10-30 00:37:55 [video_generator.py:380]                   output_path: test_sr_multi_gpu.mp4
INFO 10-30 00:37:55 [video_generator.py:380]         
[1;36m(Worker pid=13636)[0;0m INFO 10-30 00:37:55 [composed_pipeline_base.py:139] Creating pipeline stages...
[1;36m(Worker pid=13636)[0;0m INFO 10-30 00:37:55 [cuda.py:124] Trying FASTVIDEO_ATTENTION_BACKEND=VIDEO_SPARSE_ATTN
[1;36m(Worker pid=13636)[0;0m INFO 10-30 00:37:55 [cuda.py:126] Selected backend: AttentionBackendEnum.VIDEO_SPARSE_ATTN
[1;36m(Worker pid=13636)[0;0m INFO 10-30 00:37:55 [cuda.py:176] Using Video Sparse Attention backend.
[1;36m(Worker pid=13636)[0;0m INFO 10-30 00:37:55 [composed_pipeline_base.py:441] Running pipeline stages: dict_keys(['input_validation_stage', 'prompt_encoding_stage', 'conditioning_stage', 'timestep_preparation_stage', 'latent_preparation_stage', 'denoising_stage', 'decoding_stage'])
[1;36m(Worker pid=13636)[0;0m   0%|                                                                                                                                          | 0/50 [00:00<?, ?it/s]  2%|██▌                                                                                                                               | 1/50 [00:22<18:04, 22.13s/it]  4%|█████▏                                                                                                                            | 2/50 [00:42<16:58, 21.22s/it]  6%|███████▊                                                                                                                          | 3/50 [01:03<16:29, 21.05s/it]  8%|██████████▍                                                                                                                       | 4/50 [01:24<16:04, 20.96s/it] 10%|█████████████                                                                                                                     | 5/50 [01:45<15:41, 20.91s/it] 12%|███████████████▌                                                                                                                  | 6/50 [02:06<15:18, 20.88s/it] 14%|██████████████████▏                                                                                                               | 7/50 [02:26<14:56, 20.86s/it] 16%|████████████████████▊                                                                                                             | 8/50 [02:47<14:35, 20.84s/it] 18%|███████████████████████▍                                                                                                          | 9/50 [03:08<14:13, 20.81s/it][rank0]:[E1030 00:51:05.353781293 ProcessGroupNCCL.cpp:632] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1898, OpType=_ALLGATHER_BASE, NumelIn=188822272, NumelOut=377644544, Timeout(ms)=600000) ran for 600072 milliseconds before timing out.
[rank0]:[E1030 00:51:05.354242307 ProcessGroupNCCL.cpp:2271] [PG ID 8 PG GUID 18(mesh_shard) Rank 0]  failure detected by watchdog at work sequence id: 1898 PG status: last enqueued work: 1898, last completed work: 1897
[rank0]:[E1030 00:51:05.354251816 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1030 00:51:05.354287620 ProcessGroupNCCL.cpp:2106] [PG ID 8 PG GUID 18(mesh_shard) Rank 0] First PG on this rank to signal dumping.
[rank0]:[E1030 00:51:05.355537889 ProcessGroupNCCL.cpp:632] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1441, OpType=ALLTOALL_BASE, NumelIn=811008000, NumelOut=811008000, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
[rank0]:[E1030 00:51:05.355599355 ProcessGroupNCCL.cpp:2271] [PG ID 3 PG GUID 7 Rank 0]  failure detected by watchdog at work sequence id: 1441 PG status: last enqueued work: 1442, last completed work: 1440
[rank0]:[E1030 00:51:05.355609332 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1030 00:51:05.355647151 ProcessGroupNCCL.cpp:684] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E1030 00:51:05.355654869 ProcessGroupNCCL.cpp:698] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E1030 00:51:05.357070107 ProcessGroupNCCL.cpp:1899] [PG ID 3 PG GUID 7 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1441, OpType=ALLTOALL_BASE, NumelIn=811008000, NumelOut=811008000, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f16e1b785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7f16909d3a6d in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x7f16909d57f0 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f16909d6efd in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f16807d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f16e6543ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: clone + 0x44 (0x7f16e65d4a04 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 3 PG GUID 7 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1441, OpType=ALLTOALL_BASE, NumelIn=811008000, NumelOut=811008000, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f16e1b785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7f16909d3a6d in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x7f16909d57f0 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f16909d6efd in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f16807d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f16e6543ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: clone + 0x44 (0x7f16e65d4a04 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1905 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f16e1b785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x11b4abe (0x7f16909a5abe in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xe07bed (0x7f16905f8bed in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f16807d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f16e6543ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: clone + 0x44 (0x7f16e65d4a04 in /lib/x86_64-linux-gnu/libc.so.6)

Fatal Python error: Aborted

Thread 0x00007f118f7fe640 (most recent call first):
  File "/venv/main/lib/python3.10/threading.py", line 324 in wait
  File "/venv/main/lib/python3.10/threading.py", line 607 in wait
  File "/venv/main/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/venv/main/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/venv/main/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f12693ff640 (most recent call first):
  File "/venv/main/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 55 in _recv_msg
  File "/venv/main/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 191 in _read_thread
  File "/venv/main/lib/python3.10/threading.py", line 953 in run
  File "/venv/main/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/venv/main/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f16e64ac740 (most recent call first):
  File "/workspace/FastVideo/fastvideo/models/dits/wanvideo.py", line 521 in forward
  File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805 in inner
  File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857 in _call_impl
  File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751 in _wrapped_call_impl
  File "/workspace/FastVideo/fastvideo/models/dits/wanvideo.py", line 714 in forward
  File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805 in inner
  File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857 in _call_impl
  File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751 in _wrapped_call_impl
  File "/workspace/FastVideo/fastvideo/pipelines/stages/denoising.py", line 417 in forward
  File "/workspace/FastVideo/fastvideo/pipelines/stages/base.py", line 169 in __call__
  File "/workspace/FastVideo/fastvideo/pipelines/composed_pipeline_base.py", line 445 in forward
  File "/venv/main/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116 in decorate_context
  File "/workspace/FastVideo/fastvideo/worker/gpu_worker.py", line 77 in execute_forward
  File "/workspace/FastVideo/fastvideo/worker/multiproc_executor.py", line 461 in worker_busy_loop
  File "/workspace/FastVideo/fastvideo/worker/multiproc_executor.py", line 378 in worker_main
  File "/venv/main/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/venv/main/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/venv/main/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/venv/main/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>

Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, av._core, av.logging, av.bytesource, av.buffer, av.audio.format, av.error, av.dictionary, av.container.pyio, av.option, av.descriptor, av.format, av.utils, av.stream, av.container.streams, av.sidedata.motionvectors, av.sidedata.sidedata, av.opaque, av.packet, av.container.input, av.container.output, av.container.core, av.codec.context, av.video.format, av.video.reformatter, av.plane, av.video.plane, av.video.frame, av.video.stream, av.codec.hwaccel, av.codec.codec, av.frame, av.audio.layout, av.audio.plane, av.audio.frame, av.audio.stream, av.filter.link, av.filter.context, av.filter.graph, av.filter.filter, av.filter.loudnorm, av.audio.resampler, av.audio.codeccontext, av.audio.fifo, av.bitstream, av.video.codeccontext, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, regex._regex, markupsafe._speedups, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statistics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, cuda_utils, psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, ray._raylet, sentencepiece._sentencepiece, __triton_launcher (total: 167)
Traceback (most recent call last):
  File "/workspace/FastVideo/examples/test_sr_multi_gpu.py", line 33, in <module>
    main()
  File "/workspace/FastVideo/examples/test_sr_multi_gpu.py", line 19, in main
    generator.generate_video(
  File "/workspace/FastVideo/fastvideo/entrypoints/video_generator.py", line 215, in generate_video
    return self._generate_single_video(prompt=prompt,
  File "/workspace/FastVideo/fastvideo/entrypoints/video_generator.py", line 392, in _generate_single_video
    output_batch = self.executor.execute_forward(batch, fastvideo_args)
  File "/workspace/FastVideo/fastvideo/worker/multiproc_executor.py", line 72, in execute_forward
    responses = self.collective_rpc("execute_forward",
  File "/workspace/FastVideo/fastvideo/worker/multiproc_executor.py", line 148, in collective_rpc
    raise e
  File "/workspace/FastVideo/fastvideo/worker/multiproc_executor.py", line 133, in collective_rpc
    response = worker.pipe.recv()
  File "/venv/main/lib/python3.10/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/venv/main/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/venv/main/lib/python3.10/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
INFO 10-30 00:51:05 [multiproc_executor.py:155] Shutting down MultiprocExecutor...
[rank1]:[W1030 00:51:05.089896311 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=53, addr=[::ffff:127.0.0.1]:38438, remote=[::ffff:127.0.0.1]:51139): Connection reset by peer
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:675 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f15769785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7f15640d5bfe in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baafcf (0x7f15640d7fcf in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab84a (0x7f15640d884a in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7f15640d22a9 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f15257d19f9 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdc253 (0x7f15155d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f157b329ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: clone + 0x44 (0x7f157b3baa04 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W1030 00:51:05.109678455 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Connection reset by peer
[rank1]:[W1030 00:51:06.109782018 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=53, addr=[::ffff:127.0.0.1]:38438, remote=[::ffff:127.0.0.1]:51139): Broken pipe
Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f15769785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7f15640d5bfe in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baa458 (0x7f15640d7458 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5babc3e (0x7f15640d8c3e in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f15640d2298 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f15257d19f9 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdc253 (0x7f15155d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f157b329ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: clone + 0x44 (0x7f157b3baa04 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W1030 00:51:06.139288078 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe
[rank1]:[W1030 00:51:07.139391790 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=53, addr=[::ffff:127.0.0.1]:38438, remote=[::ffff:127.0.0.1]:51139): Broken pipe
Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f15769785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7f15640d5bfe in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baa458 (0x7f15640d7458 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5babc3e (0x7f15640d8c3e in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f15640d2298 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f15257d19f9 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdc253 (0x7f15155d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f157b329ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: clone + 0x44 (0x7f157b3baa04 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W1030 00:51:07.168438244 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe
[rank1]:[W1030 00:51:08.168547306 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=53, addr=[::ffff:127.0.0.1]:38438, remote=[::ffff:127.0.0.1]:51139): Broken pipe
Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f15769785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7f15640d5bfe in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baa458 (0x7f15640d7458 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5babc3e (0x7f15640d8c3e in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f15640d2298 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f15257d19f9 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdc253 (0x7f15155d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f157b329ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: clone + 0x44 (0x7f157b3baa04 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W1030 00:51:08.188024753 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe
[rank1]:[W1030 00:51:09.188237969 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=53, addr=[::ffff:127.0.0.1]:38438, remote=[::ffff:127.0.0.1]:51139): Broken pipe
Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f15769785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7f15640d5bfe in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baa458 (0x7f15640d7458 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5babc3e (0x7f15640d8c3e in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f15640d2298 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f15257d19f9 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdc253 (0x7f15155d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f157b329ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: clone + 0x44 (0x7f157b3baa04 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W1030 00:51:09.207889571 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe
[rank1]:[W1030 00:51:10.208054324 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=53, addr=[::ffff:127.0.0.1]:38438, remote=[::ffff:127.0.0.1]:51139): Broken pipe
Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f15769785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7f15640d5bfe in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baa458 (0x7f15640d7458 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5babc3e (0x7f15640d8c3e in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f15640d2298 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f15257d19f9 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdc253 (0x7f15155d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f157b329ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: clone + 0x44 (0x7f157b3baa04 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W1030 00:51:10.227457760 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe
[rank1]:[W1030 00:51:11.227565451 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=53, addr=[::ffff:127.0.0.1]:38438, remote=[::ffff:127.0.0.1]:51139): Broken pipe
Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f15769785e8 in /venv/main/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7f15640d5bfe in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baa458 (0x7f15640d7458 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5babc3e (0x7f15640d8c3e in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f15640d2298 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f15257d19f9 in /venv/main/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdc253 (0x7f15155d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f157b329ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: clone + 0x44 (0x7f157b3baa04 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W1030 00:51:11.259326289 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe
INFO 10-30 00:51:17 [multiproc_executor.py:208] MultiprocExecutor shutdown complete
/venv/main/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
